{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7389a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c82b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRelu:\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def output(self, x):\n",
    "        return np.maximum(x*self.alpha, x)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0, 1, self.alpha) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1cec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_vec = np.empty(input_size)\n",
    "        self.output = np.empty(1)\n",
    "        # dtype is np.float64 by default\n",
    "        self.weights = np.random.uniform(-0.3, 0.3, input_size)\n",
    "        self.local_grad = np.empty(1)\n",
    "    \n",
    "    def update_weights(self, etta):\n",
    "        delta_weights = np.multiply(etta, self.local_grad)\n",
    "        self.weights = self.weights + delta_weights\n",
    "        \n",
    "    def call(self, input_vector):\n",
    "        self.input_vec = input_vector\n",
    "        self.output = np.dot(input_vector, self.weights)\n",
    "        return self.output\n",
    "    \n",
    "    @property\n",
    "    def w(self):\n",
    "        return self.weights[1:]\n",
    "    \n",
    "    @property  \n",
    "    def b(self):\n",
    "        return self.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29ce0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer:\n",
    "    def __init__(self, input_dimension: int, output_dimension: int, activation_function, bias=True):\n",
    "        # bias is True by default and in this code I did not change it for hidden layers\n",
    "        self.input_dimension = input_dimension+1\n",
    "        self.neuron_count = output_dimension+1 if bias else output_dimension\n",
    "        self.neurons = [Neuron(self.input_dimension) for _ in range(self.neuron_count)]\n",
    "        self.a_f = activation_function\n",
    "        self.localgrads_in_w = np.empty((self.neuron_count, self.input_dimension))\n",
    "    \n",
    "    def calculate_outputs(self, x_input):\n",
    "        outputs = []      \n",
    "        for neuron in self.neurons:\n",
    "            output = self.a_f.output(neuron.call(x_input))\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return np.array(outputs)\n",
    "    \n",
    "    def calculate_outputlayer_localgrad(self, loss):\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron.local_grad = np.multiply(loss, self.a_f.derivative(neuron.output))\n",
    "            self.localgrads_in_w[i] = (np.multiply(neuron.local_grad, neuron.weights))\n",
    "        \n",
    "    def calculate_hiddenlayer_localgrad(self, next_layer):\n",
    "        sum_nexlayer_localgrads = np.sum(next_layer.localgrads_in_w, axis=0)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron.local_grad = np.multiply(self.a_f.derivative(neuron.output), sum_nexlayer_localgrads[i])\n",
    "    \n",
    "    def update_weights(self, etta):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_weights(etta)\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [neuron.weights for neuron in self.neurons]\n",
    "    \n",
    "    @property\n",
    "    def neuron_list(self):\n",
    "        return list(self.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a211bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.neural_layers = []\n",
    "    \n",
    "    def sequential(self, neural_layers: list):\n",
    "        self.neural_layers = neural_layers\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.neural_layers:\n",
    "            layer.zero_grad()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        input_vector = np.concatenate((np.array(X), np.array([1])), axis=0)\n",
    "        for layer in self.neural_layers:\n",
    "            output_vector = layer.calculate_outputs(input_vector)\n",
    "            input_vector = output_vector\n",
    "        return output_vector\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        self.neural_layers[-1].calculate_outputlayer_localgrad(loss)\n",
    "        \n",
    "        reverse_order = self.neural_layers[::-1]\n",
    "        for i, layer in enumerate(reverse_order[1:], 1): \n",
    "            next_layer = reverse_order[i-1]\n",
    "            layer.calculate_hiddenlayer_localgrad(next_layer)\n",
    "    \n",
    "    def update_weights(self, etta):\n",
    "        for layer in self.neural_layers:\n",
    "            layer.update_weights(etta)\n",
    "    \n",
    "    \n",
    "    def loss(self, desired_output, predicted_output):\n",
    "        return np.mean(np.subtract(np.array(desired_output), np.array(predicted_output)))\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.neural_layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "520c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.sequential([\n",
    "    NeuralLayer(4,8, LeakyRelu()),\n",
    "    NeuralLayer(8,12, LeakyRelu()),\n",
    "    NeuralLayer(12,1, LeakyRelu(), bias=False),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0b6f4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_parameters\n",
    "num_epochs = 12\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ce5b1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = list(iris[\"data\"])\n",
    "y = list((iris[\"target\"]).astype(np.int64))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4)\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    for i in range(len(y_train)):\n",
    "        y_pred = model.forward(X_train[i])\n",
    "        loss = model.loss(y_train[i], y_pred)\n",
    "        model.backward(loss)\n",
    "        model.update_weights(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "033472b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166666666666667"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_predict = [int(model.forward(j)) for j in X_test]\n",
    "accuracy_score(y_test, y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
