{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f7389a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d99c4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "38c82b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRelu:\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def output(self, x):\n",
    "        return np.maximum(x*self.alpha, x)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0, 1, self.alpha) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4c1cec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        # dtype is np.float64 by default\n",
    "        self.weights = np.random.uniform(-0.3, 0.3, input_size)\n",
    "        # this is array actually stores --> derivative(phi(x)) * y\n",
    "        self.grad_tape = np.zeros(input_size)\n",
    "    \n",
    "    def update_weights(self, delta_weights):\n",
    "        self.weights = self.weights + delta_weights\n",
    "        \n",
    "    def zero_gradient(self):\n",
    "        self.grad_tape = np.zeros(self.grad_tape.size)\n",
    "        \n",
    "    def call(self, input_vector):\n",
    "        return np.dot(input_vector, self.weights)\n",
    "    \n",
    "    @property\n",
    "    def w(self):\n",
    "        return self.weights[1:]\n",
    "    \n",
    "    @property  \n",
    "    def b(self):\n",
    "        return self.weights[0]\n",
    "    \n",
    "    @property\n",
    "    def gradient_tape(self, w=None):\n",
    "        return self.grad_tape[w] if w else self.grad_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d29ce0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer:\n",
    "    def __init__(self, input_dimension: int, output_dimension: int, activation_function, bias=True):\n",
    "        # bias is True by default and in this code I did not change it for hidden layers\n",
    "        self.input_dimension = input_dimension+1\n",
    "        self.neuron_count = output_dimension+1 if bias else output_dimension\n",
    "        self.neurons = [Neuron(self.input_dimension) for _ in range(self.neuron_count)]\n",
    "        self.a_f = activation_function\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.zero_gradient()\n",
    "    \n",
    "    def calculate_outputs(self, x_input):\n",
    "        outputs = []\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            output = self.a_f.output(neuron.call(x_input))\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # stores ---> derivative(phi(x)) * y\n",
    "            phi_prime = self.a_f.derivative(neuron.call(x_input))\n",
    "            neuron.grad_tape = (phi_prime * np.array(x_input))\n",
    "        \n",
    "        return np.array(outputs)\n",
    "            \n",
    "            \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [neuron.weights for neuron in self.neurons]\n",
    "    \n",
    "    @property\n",
    "    def neuron_list(self):\n",
    "        return list(self.neurons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a211bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.neural_layers = []\n",
    "        self.learning_rate = 0.05\n",
    "    \n",
    "    def sequential(self, neural_layers: list):\n",
    "        self.neural_layers = neural_layers\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.neural_layers:\n",
    "            layer.zero_grad()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        input_vector = np.concatenate((np.array(X), np.array([1])), axis=0)\n",
    "        for layer in self.neural_layers:\n",
    "            output_vector = layer.calculate_outputs(input_vector)\n",
    "            input_vector = output_vector\n",
    "        return output_vector\n",
    "    \n",
    "    def loss(self, desired_output, predicted_output):\n",
    "        return np.subtract(np.array(desired_output), np.array(predicted_output))\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.neural_layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "520c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.learning_rate = 0.05\n",
    "model.sequential([\n",
    "    NeuralLayer(2,3, LeakyRelu()),\n",
    "    NeuralLayer(3,2, LeakyRelu(), bias=False),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2fc5ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.forward(np.array([1,2]))\n",
    "# print(y_pred)\n",
    "# model.loss([1,1], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd3a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
