{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f7389a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "38c82b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRelu:\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def output(self, x):\n",
    "        return np.maximum(x*self.alpha, x)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0, 1, self.alpha) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "4c1cec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_vec = np.empty(input_size)\n",
    "        self.output = np.empty(1)\n",
    "        # dtype is np.float64 by default\n",
    "        self.weights = np.random.uniform(-0.3, 0.3, input_size)\n",
    "        self.local_grad = np.empty(1)\n",
    "    \n",
    "    def update_weights(self, etta):\n",
    "        delta_weights = np.multiply(etta, self.local_grad)\n",
    "        self.weights = self.weights + delta_weights\n",
    "        \n",
    "    def call(self, input_vector):\n",
    "        self.input_vec = input_vector\n",
    "        self.output = np.dot(input_vector, self.weights)\n",
    "        return self.output\n",
    "    \n",
    "    @property\n",
    "    def w(self):\n",
    "        return self.weights[1:]\n",
    "    \n",
    "    @property  \n",
    "    def b(self):\n",
    "        return self.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d29ce0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer:\n",
    "    def __init__(self, input_dimension: int, output_dimension: int, activation_function, bias=True):\n",
    "        # bias is True by default and in this code I did not change it for hidden layers\n",
    "        self.input_dimension = input_dimension+1\n",
    "        self.neuron_count = output_dimension+1 if bias else output_dimension\n",
    "        self.neurons = [Neuron(self.input_dimension) for _ in range(self.neuron_count)]\n",
    "        self.a_f = activation_function\n",
    "        self.localgrads_in_w = np.empty((self.neuron_count, self.input_dimension))\n",
    "    \n",
    "    def calculate_outputs(self, x_input):\n",
    "        outputs = []      \n",
    "        for neuron in self.neurons:\n",
    "            output = self.a_f.output(neuron.call(x_input))\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return np.array(outputs)\n",
    "    \n",
    "    def calculate_outputlayer_localgrad(self, loss):\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron.local_grad = np.multiply(loss, self.a_f.derivative(neuron.output))\n",
    "            self.localgrads_in_w[i] = (np.multiply(neuron.local_grad, neuron.weights))\n",
    "        \n",
    "    def calculate_hiddenlayer_localgrad(self, next_layer):\n",
    "        sum_nexlayer_localgrads = np.sum(next_layer.localgrads_in_w, axis=0)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron.local_grad = np.multiply(self.a_f.derivative(neuron.output), sum_nexlayer_localgrads[i])\n",
    "    \n",
    "    def update_weights(self, etta):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_weights(etta)\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [neuron.weights for neuron in self.neurons]\n",
    "    \n",
    "    @property\n",
    "    def neuron_list(self):\n",
    "        return list(self.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a211bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.neural_layers = []\n",
    "    \n",
    "    def sequential(self, neural_layers: list):\n",
    "        self.neural_layers = neural_layers\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for layer in self.neural_layers:\n",
    "            layer.zero_grad()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        input_vector = np.concatenate((np.array(X), np.array([1])), axis=0)\n",
    "        for layer in self.neural_layers:\n",
    "            output_vector = layer.calculate_outputs(input_vector)\n",
    "            input_vector = output_vector\n",
    "        return output_vector\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        self.neural_layers[-1].calculate_outputlayer_localgrad(loss)\n",
    "        \n",
    "        reverse_order = self.neural_layers[::-1]\n",
    "        for i, layer in enumerate(reverse_order[1:], 1): \n",
    "            next_layer = reverse_order[i-1]\n",
    "            layer.calculate_hiddenlayer_localgrad(next_layer)\n",
    "    \n",
    "    def update_weights(self, etta):\n",
    "        for layer in self.neural_layers:\n",
    "            layer.update_weights(etta)\n",
    "    \n",
    "    \n",
    "    def loss(self, desired_output, predicted_output):\n",
    "        return np.mean(np.subtract(np.array(desired_output), np.array(predicted_output)))\n",
    "    \n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.neural_layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "520c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.sequential([\n",
    "    NeuralLayer(2,3, LeakyRelu()),\n",
    "    NeuralLayer(3,2, LeakyRelu(), bias=False),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "0b6f4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_parameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "033472b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9898213275080514\n",
      "0.9823955429097722\n",
      "0.9688867222457107\n",
      "0.9436886896237235\n",
      "0.9022311414157838\n",
      "0.838656750997308\n",
      "0.6740262398360437\n",
      "0.4996334864689257\n",
      "0.3430840179044615\n",
      "0.2209158826518658\n",
      "0.1355373826577284\n",
      "0.08044819230196587\n",
      "0.04675208479344639\n",
      "0.026823112207944977\n",
      "0.01527320652937092\n",
      "0.00865864117583448\n",
      "0.004896450670173524\n",
      "0.0027649967621619087\n",
      "0.0015601186689720992\n",
      "0.0008798784746727462\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_epochs):\n",
    "    y_pred = model.forward(np.array([1,2]))\n",
    "    loss = model.loss([1,1], y_pred)\n",
    "    print(loss)\n",
    "    model.backward(loss)\n",
    "    model.update_weights(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e2f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
